{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cruncher\n",
    "### Amazon Advertising Data Aggregator and Bid Adjustment catalyst\n",
    "The goal of this tool is to merge various data sources into succinct and focused PPC Audit spreadsheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import sys\n",
    "import os\n",
    "import glob\n",
    "import openpyxl\n",
    "\n",
    "from openpyxl import Workbook,load_workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "from numpy.polynomial import Polynomial as Poly\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "# Create Time Stamp\n",
    "today = date.today()\n",
    "td_form = today.strftime(\"%b-%d-%Y\")\n",
    "\n",
    "# Set option to regard empty string as NaN\n",
    "pd.set_option('mode.use_inf_as_na', True)\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for File management\n",
    "\n",
    "#Fn to read a tab of the bulksheet into a df\n",
    "def readbs(path,tab):\n",
    "    newpath = f'{path}*.xlsx'\n",
    "    this = glob.glob(newpath)\n",
    "    if (len(this)>1):\n",
    "        print(f'Warn: Too many bulksheet file ({len(this)}).. Using first item..')\n",
    "    return pd.read_excel(this[0], tab)\n",
    "\n",
    "def readbs_sp(path,tab):\n",
    "    newpath = f'{path}*.xlsx'\n",
    "    this = glob.glob(newpath)\n",
    "    if (len(this)>1):\n",
    "        print(f'Warn: Too many bulksheet file ({len(this)}).. Using first item..')\n",
    "    return pd.read_excel(this[0], tab, dtype={'Campaign Id':'string','Ad Group Id':'string',\n",
    "                'Keyword Id (Read only)':'string','Product Targeting Id (Read only)':'string'})\n",
    "\n",
    "#Fn to read the first sheet of a single excel doc with a folder reference\n",
    "def readis(path,enc):\n",
    "    newpath = f'{path}*.csv'\n",
    "    this = glob.glob(newpath)\n",
    "    if (len(this)>1):\n",
    "        print(f'Warn: Too many csv file ({len(this)}).. Using first item..')\n",
    "    return pd.read_csv(this[0], encoding=enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for Accepting User Input\n",
    "\n",
    "#Fn for onboarding new ad group detected in bulksheet\n",
    "def new_ag(aga):\n",
    "    new_aga = None\n",
    "    for idx, row in aga.iterrows():\n",
    "        add = row\n",
    "        add['Product Name'] = add['Ad Group Name'].split(' - ')[0]\n",
    "        p = f'New Ad Group Detected: {os.linesep} {add[\"Ad Group Name\"]} {os.linesep} Would you like to add it? .. (Y/nae)'\n",
    "        r = input(p)\n",
    "        if r == 'y' or r == 'Y' or r == 'yae' or r == 'Yae' or r == 'Yes' or r == 'yes':\n",
    "            p = f'Enter Ad Group Type: {os.linesep} (i.e KW Broad, KW Phrase KW Exact, AUTO, CAT, ASIN Anc, ASIN Comp) {os.linesep} ..'\n",
    "            add['Ad Group Type'] = input(p)\n",
    "            p = f'Enter Product Name: {os.linesep} Expected:{add[\"Product Name\"]}..'\n",
    "            add['Product Name'] = input(p)\n",
    "            if new_aga is None:\n",
    "                new_aga = add\n",
    "            else:\n",
    "                new_aga = pd.concat([new_aga, add], axis=1)\n",
    "    return pd.DataFrame(new_aga).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Paths..\n"
     ]
    }
   ],
   "source": [
    "# Paths Setup\n",
    "print('Updating Paths..')\n",
    "\n",
    "ddir = 'Data/'\n",
    "rdir = 'Resources/'\n",
    "bsdir = f'{ddir}Bulksheet/'\n",
    "h10dir = f'{ddir}H10_Keyword_History/'\n",
    "spdir = f'{ddir}SP_Reports/'\n",
    "sbdir = f'{ddir}SB_Reports/'\n",
    "sbvdir = f'{ddir}SBV_Reports/'\n",
    "confdir = f'{rdir}Config/'\n",
    "hist_path = f'{rdir}History/adjustment_history.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Bulksheet for Sponsored Products..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kneel/.local/lib/python3.10/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Targeting 90D for Sponsored Products..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kneel/.local/lib/python3.10/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Searchterm IS for Sponsored Products..\n"
     ]
    }
   ],
   "source": [
    "# Pull in Current Sponsored Products Reports\n",
    "print('Updating Bulksheet for Sponsored Products..')\n",
    "raw_sp = readbs_sp(bsdir,'Sponsored Products Campaigns')\n",
    "\n",
    "print('Updating Targeting 90D for Sponsored Products..')\n",
    "sp_targ = readbs(f'{spdir}90D_Targeting/',0)\n",
    "\n",
    "print('Updating Searchterm IS for Sponsored Products..')\n",
    "sp_is = readis(f'{spdir}30D_Searchterm_IS/','UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Bulksheet for Sponsored BRANDS..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kneel/.local/lib/python3.10/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Targeting 90D for Sponsored Brands..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kneel/.local/lib/python3.10/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Targeting 90D for Sponsored Brands Video..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kneel/.local/lib/python3.10/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Searchterm IS for Sponsored Brands..\n"
     ]
    }
   ],
   "source": [
    "# Pull in Current Sponsored Brands Reports\n",
    "print('Updating Bulksheet for Sponsored BRANDS..')\n",
    "raw_sb = readbs_sp(bsdir,'Sponsored Brands Campaigns')\n",
    "\n",
    "print('Updating Targeting 90D for Sponsored Brands..')\n",
    "sb_targ = readbs(f'{sbdir}90D_Keyword/',0)\n",
    "\n",
    "print('Updating Targeting 90D for Sponsored Brands Video..')\n",
    "sbv_targ = readbs(f'{sbvdir}90D_Keyword/',0)\n",
    "\n",
    "print('Updating Searchterm IS for Sponsored Brands..')\n",
    "sb_is = readis(f'{sbdir}30D_Searchterm_IS/','UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new ad groups..\n",
      "Updating Product Info..\n",
      "Updating Bid Multipliers..\n",
      "Writing Updates to AG config..\n"
     ]
    }
   ],
   "source": [
    "# Import Config Csvs\n",
    "prod_df = pd.read_csv(f'{confdir}products.csv', dtype={'Product Name':'string', \n",
    "       'Ad Type':'string', 'Campaign Id':'string', 'Ad Group Id':'string',\n",
    "       'Campaign Name':'string', 'Ad Group Name':'string', 'Default Bid':'float64', 'Ad Group Type':'string',\n",
    "       'ASIN':'string', 'SKU':'string', 'Total Cost':'float64', 'Pick Pack Fee':'float64', 'AMZ Price':'float64',\n",
    "       'Referral Fee':'float64', 'BE ROAS':'float64', 'BE ACOS':'float64', 'Percentage':'float64', 'DP CVR':'float64'})\n",
    "ag_df = pd.read_csv(f'{confdir}adgroups.csv', dtype={'Product Name':'string', \n",
    "       'Ad Type':'string', 'Campaign Id':'string', 'Ad Group Id':'string',\n",
    "       'Campaign Name':'string', 'Ad Group Name':'string', 'Default Bid':'float64', 'Ad Group Type':'string',\n",
    "       'ASIN':'string', 'SKU':'string', 'Total Cost':'float64', 'Pick Pack Fee':'float64', 'AMZ Price':'float64',\n",
    "       'Referral Fee':'float64', 'BE ROAS':'float64', 'BE ACOS':'float64', 'Percentage':'float64', 'DP CVR':'float64'})\n",
    "\n",
    "\n",
    "# Updates to Ad Group Config\n",
    "\n",
    "# Check for new ad groups in bulk sheet for existing products\n",
    "ag_add = raw_sp.loc[(raw_sp['Entity'] == 'Ad Group') & (raw_sp['Ad Group Id'].isin(ag_df['Ad Group Id']) == False),['Product','Campaign Id','Ad Group Id', 'Campaign Name (Informational only)', 'Ad Group Name (Informational only)', 'Ad Group Default Bid']].reset_index(drop=True)\n",
    "ag_add.columns = ['Ad Type','Campaign Id','Ad Group Id','Campaign Name','Ad Group Name','Default Bid']\n",
    "ag_add_sb = raw_sb.loc[(raw_sb['Entity'] == 'Campaign') & (raw_sb['Campaign Id'].isin(ag_df['Campaign Id']) == False) & (raw_sb['Campaign Name'].isin(ag_df['Campaign Name']) == False),['Product','Campaign Id','Ad Group Id (Read only)', 'Campaign Name (Informational only)', 'Campaign Name (Informational only)', 'Bid']].reset_index(drop=True)\n",
    "ag_add_sb.columns = ['Ad Type','Campaign Id','Ad Group Id','Campaign Name','Ad Group Name','Default Bid']\n",
    "ag_add = pd.concat([ag_add,ag_add_sb], axis=0)\n",
    "\n",
    "row_no= ag_add.shape[0]\n",
    "if (row_no>0):\n",
    "    print(f'Warn: {row_no} new ad groups detected in campaigns.. Need to add to config to process..')\n",
    "\n",
    "    screened = new_ag(ag_add)\n",
    "    if screened is not None:\n",
    "       ag_df = pd.concat([ag_df,screened])\n",
    "else:\n",
    "    print('No new ad groups..')\n",
    "\n",
    "print('Updating Product Info..')\n",
    "# Merge with Latest Product Info\n",
    "#ag_df = pd.merge(ag_df,prod_df, on='Product Name', how='left').reset_index(drop=True)\n",
    "ag_df = ag_df.set_index('Product Name')\n",
    "ag_df.update(prod_df.set_index('Product Name'))\n",
    "ag_df = ag_df.reset_index()\n",
    "\n",
    "print('Updating Bid Multipliers..')\n",
    "# First Get Bid Multipliers\n",
    "mult = raw_sp[(raw_sp.Entity == 'Bidding Adjustment')].groupby(['Campaign Id']).Percentage.max()\n",
    "# Merge multipliers to ad groups dataframe in memory\n",
    "ag_df = ag_df.merge(mult, how='left', on='Campaign Id').reset_index(drop=True)\n",
    "ag_df.Percentage_y = (ag_df.Percentage_y/100) + 1\n",
    "ag_df['Percentage'] = ag_df.Percentage_y.fillna(ag_df.Percentage_x)\n",
    "\n",
    "ag_df = ag_df.drop(['Percentage_x', 'Percentage_y'], axis=1)\n",
    "\n",
    "ag_df.Percentage = ag_df.Percentage.fillna(1)\n",
    "\n",
    "print('Writing Updates to AG config..')\n",
    "ag_df.to_csv(f'{confdir}adgroups.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File read success..\n",
      "finish concat.. cleaning dupes..\n",
      "Combining Ranks to comma delimited timeseries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10649/3345205729.py:51: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  rank_df = rank_df.sort_values('Date Added',ascending=False).groupby(['ASIN','Keyword'])['Search Volume','Organic Rank','Sponsored Position'].apply(rankagg)\n"
     ]
    }
   ],
   "source": [
    "#Converter for coercing datatype in h10 export\n",
    "def rank_conv(v):\n",
    "    try:\n",
    "        return int(v)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "# Function for reading in H10 Rank csv files all at once\n",
    "def rankfolder(path,enc,sub):\n",
    "    dfa = None\n",
    "    newpath = f'{path}*.csv'\n",
    "    for f in glob.glob(newpath):\n",
    "        if dfa is None:\n",
    "            dfa = pd.read_csv(f, encoding=enc, converters={'Search Volume':rank_conv}, low_memory=False, parse_dates=['Date Added'])\n",
    "            print('File read success..')\n",
    "        else:\n",
    "            dfb = pd.read_csv(f, encoding=enc, converters={'Search Volume':rank_conv}, low_memory=False, parse_dates=['Date Added'])\n",
    "            dfa = pd.concat([dfa, dfb])\n",
    "            #print('Concat Next File success..')\n",
    "    print('finish concat.. cleaning dupes..')\n",
    "    #print(dfa.head())\n",
    "    return dfa.drop_duplicates(subset=sub).reset_index()\n",
    "\n",
    "#Set subset for duplicate search\n",
    "rank_subset = ['ASIN','Keyword','Date Added']\n",
    "\n",
    "#Read in CSVS\n",
    "rank_df = rankfolder(h10dir,'UTF-8',rank_subset)\n",
    "\n",
    "\n",
    "# Function to parse groupby in preparation to merge\n",
    "def rankagg(g):\n",
    "    org = pd.Series(x for  x in g['Organic Rank'] if x !='>306').astype('string')\n",
    "    spo = pd.Series(x for  x in g['Sponsored Position'] if x !='-').astype('string')\n",
    "    length = spo.size\n",
    "    if spo is not None and (length > 1):\n",
    "        sp_slope = np.polyfit(np.arange(0,length,1),spo.astype('float'),deg=1)[0]\n",
    "    else:\n",
    "        sp_slope = np.nan\n",
    "    return pd.Series({\n",
    "        'Search Volume': g['Search Volume'].mean(),\n",
    "        'Organic Rank': (', ').join(org),\n",
    "        'Sponsored Rank': (', ').join(spo),\n",
    "        'Organic Mean':org.astype('float').mean(),\n",
    "        'Sponsored Mean':spo.astype('float').mean(),\n",
    "        'Sponsored Slope':sp_slope\n",
    "    })\n",
    "\n",
    "# Group by unique Keyword/Product Combo and apply function to concat ranks\n",
    "print('Combining Ranks to comma delimited timeseries')\n",
    "rank_df = rank_df.sort_values('Date Added',ascending=False).groupby(['ASIN','Keyword'])['Search Volume','Organic Rank','Sponsored Position'].apply(rankagg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10649/2375991302.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sp_targ_clean['st_resolve'] = sp_targ_clean.apply(lambda x: re.compile(r'asin=\"([^\"]*)\"', re.IGNORECASE).match(x['Targeting']).group(1).lower() if re.compile(r'asin=\"([^\"]*)\"', re.IGNORECASE).match(x['Targeting']) else x['Targeting'], axis=1)\n",
      "/tmp/ipykernel_10649/2375991302.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sp_targ_clean['Match Type'] = sp_targ_clean['Match Type'].str.lower()\n",
      "/tmp/ipykernel_10649/2375991302.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sp_targ_clean.drop('Targeting', axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# The Big Merge (SP)\n",
    "\n",
    "# Merge BS and Ad Group info\n",
    "merged = pd.merge(raw_sp[['Product', 'Entity', 'Campaign Id', 'Ad Group Id',\n",
    "       'Keyword Id (Read only)', 'Product Targeting Id (Read only)',\n",
    "       'State', 'Campaign State (Informational only)',\n",
    "       'Ad Group State (Informational only)',  \n",
    "       'Ad Group Default Bid (Informational only)',\n",
    "       'Bid', 'Keyword Text', 'Match Type', 'Product Targeting Expression',\n",
    "       'Impressions', 'Clicks', 'Click-through Rate', 'Spend', 'Sales',\n",
    "       'Orders', 'Units', 'Conversion Rate', 'CPC', 'ROAS']],\n",
    "       ag_df[['Ad Group Id', 'Product Name', 'Ad Type', 'Campaign Name',\n",
    "       'Ad Group Name', 'Ad Group Type', 'ASIN', 'SKU','AMZ Price', 'BE ROAS',\n",
    "       'BE ACOS', 'Percentage', 'Organic Target', 'Sponsored Target', 'DP CVR']], \n",
    "       on='Ad Group Id', how='left')\n",
    "\n",
    "# Drop Rows without Targets\n",
    "merged = merged[(merged.State == 'enabled') & (merged['Campaign State (Informational only)'] == 'enabled') & (merged['Ad Group State (Informational only)'] == 'enabled') & ((merged.Entity == 'Keyword') | (merged.Entity == 'Product Targeting'))]\n",
    "\n",
    "#Resolve match type for products to match index of targeting report\n",
    "merged['Match Type'] = merged['Match Type'].fillna('-')\n",
    "# Add column to help match with st_is report\n",
    "merged['st_resolve'] = merged.apply(lambda x: x['Keyword Text'] if x['Entity'] == 'Keyword' else re.compile(r'asin=\"([^\"]*)\"', re.IGNORECASE).match(x['Product Targeting Expression']).group(1).lower() if re.compile(r'asin=\"([^\"]*)\"', re.IGNORECASE).match(x['Product Targeting Expression']) else np.nan, axis=1) \n",
    "merged['bid_resolve'] = merged.Bid.fillna(merged['Ad Group Default Bid (Informational only)'])\n",
    "# Drop Rows in Searchterm IS that don't help with bid audits\n",
    "sp_is_clean = pd.merge(sp_is[['Customer Search Term',\n",
    "       'Search Term Impression Rank', 'Search Term Impression Share','Campaign Name','Ad Group Name']],\n",
    "       ag_df[['Campaign Name','Ad Group Name','Ad Group Type']], on=['Campaign Name','Ad Group Name'], how='left')\n",
    "sp_is_clean = sp_is_clean[(sp_is_clean['Ad Group Type'] == 'KW Exact') | (sp_is_clean['Ad Group Type'] == 'ASIN Comp') | (sp_is_clean['Ad Group Type'] == 'ASIN Anc')].drop('Ad Group Type',axis=1)\n",
    "sp_is_clean['Search Term Impression Share'] = sp_is_clean['Search Term Impression Share'].str.replace('%','').astype('float64')/100\n",
    "sp_is_clean['Search Term Impression Rank'] = sp_is_clean['Search Term Impression Rank'].astype('int')\n",
    "\n",
    "# Merge BS and Searchterm IS Report\n",
    "merged = pd.merge(merged, sp_is_clean, left_on=['Campaign Name','Ad Group Name','st_resolve'], right_on=['Campaign Name','Ad Group Name','Customer Search Term'], how='left').drop('Customer Search Term', axis=1)\n",
    "\n",
    "# Calculate ceiling bids using listing conversion rates and multipliers\n",
    "# (if no DP CVR OR current bid is already over this number - we set a fallback of $6 just to avoid obscenely high system recs)\n",
    "merged['bid_ceil'] = merged.apply(lambda x: ((x['DP CVR']*x['AMZ Price']*x['BE ACOS'])/x.Percentage) if (x['DP CVR'] is not None and (((x['DP CVR']*x['AMZ Price']*x['BE ACOS'])/x.Percentage) >= x.bid_resolve)) else (6/x.Percentage),axis=1)\n",
    "\n",
    "# Drop Columns Unnecessary in Targeting Report\n",
    "sp_targ_clean = sp_targ[['Campaign Name','Ad Group Name','Targeting','Match Type','Impressions','Cost Per Click (CPC)','Click-Thru Rate (CTR)','7 Day Conversion Rate','Total Return on Advertising Spend (ROAS)']]\n",
    "sp_targ_clean.columns = ['Campaign Name','Ad Group Name','Targeting','Match Type','90D Impressions','90D CPC','90D CTR','90D CVR','90D ROAS']\n",
    "sp_targ_clean['st_resolve'] = sp_targ_clean.apply(lambda x: re.compile(r'asin=\"([^\"]*)\"', re.IGNORECASE).match(x['Targeting']).group(1).lower() if re.compile(r'asin=\"([^\"]*)\"', re.IGNORECASE).match(x['Targeting']) else x['Targeting'], axis=1)\n",
    "sp_targ_clean['Match Type'] = sp_targ_clean['Match Type'].str.lower()\n",
    "s\n",
    "sp_targ_clean.drop('Targeting', axis=1, inplace=True)\n",
    "\n",
    "# Merge BS and Longterm Targeting KPI\n",
    "merged = pd.merge(merged, sp_targ_clean, on=['Campaign Name','Ad Group Name','st_resolve','Match Type'], how='left')\n",
    "\n",
    "# Add merge key to rank df to indicate only KW Exacts\n",
    "rank_df['Ad Group Type'] = rank_df.apply(lambda x: 'KW Exact',axis=1)\n",
    "\n",
    "#Merge BS and rank df\n",
    "merged = pd.merge(merged,rank_df,left_on=['ASIN','Keyword Text','Ad Group Type'],right_on=['ASIN','Keyword','Ad Group Type'],how='left').drop_duplicates(subset=['Keyword Id (Read only)', 'Product Targeting Id (Read only)', 'st_resolve','Match Type','bid_resolve']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big Merge 2 (SB and SBV)\n",
    "\n",
    "# Merge BS and Ad Group info\n",
    "\n",
    "merged_sb = pd.merge(raw_sb[['Product', 'Entity', 'Campaign Id','Campaign Serving Status (Informational only)',\n",
    "       'Keyword Id (Read only)', 'Product Targeting Id (Read only)',\n",
    "       'State', 'Bid', 'Keyword Text', 'Match Type', 'Product Targeting Expression',\n",
    "       'Impressions', 'Clicks', 'Click-through Rate', 'Spend', 'Sales',\n",
    "       'Orders', 'Units', 'Conversion Rate', 'CPC', 'ROAS']],\n",
    "       ag_df[['Campaign Id', 'Product Name', 'Ad Type', 'Campaign Name',\n",
    "       'Ad Group Name', 'Ad Group Type', 'ASIN', 'SKU','AMZ Price', 'BE ROAS',\n",
    "       'BE ACOS', 'Percentage', 'DP CVR']], \n",
    "       on='Campaign Id', how='left')\n",
    "\n",
    "# Drop Rows without Targets\n",
    "merged_sb = merged_sb[(merged_sb.State == 'enabled')  & ((merged_sb['Campaign Serving Status (Informational only)'] == 'running') | (merged_sb['Campaign Serving Status (Informational only)'] == 'other')) & ((merged_sb.Entity == 'Keyword') | (merged_sb.Entity == 'Product Targeting'))]\n",
    "\n",
    "# Add column to help match with st_is report\n",
    "merged_sb['st_resolve'] = merged_sb.apply(lambda x: x['Keyword Text'] if x['Entity'] == 'Keyword' else re.compile(r'asin=\"([^\"]*)\"', re.IGNORECASE).match(x['Product Targeting Expression']).group(1).lower() if re.compile(r'asin=\"([^\"]*)\"', re.IGNORECASE).match(x['Product Targeting Expression']) else np.nan, axis=1) \n",
    "merged_sb['bid_resolve'] = merged_sb.Bid\n",
    "\n",
    "# Drop Rows in Searchterm IS that don't help with bid audits\n",
    "sb_is_clean = pd.merge(sb_is[['Customer Search Term',\n",
    "       'Search Term Impression Rank', 'Search Term Impression Share','Campaign Name']],\n",
    "       ag_df[['Campaign Name','Ad Group Type']], on=['Campaign Name'], how='left')\n",
    "sb_is_clean = sb_is_clean[(sb_is_clean['Ad Group Type'] == 'KW Exact') | (sb_is_clean['Ad Group Type'] == 'ASIN Comp') | (sb_is_clean['Ad Group Type'] == 'ASIN Anc')].drop('Ad Group Type',axis=1)\n",
    "sb_is_clean['Search Term Impression Share'] = sb_is_clean['Search Term Impression Share'].str.replace('%','').astype('float64')/100\n",
    "sb_is_clean['Search Term Impression Rank'] = sb_is_clean['Search Term Impression Rank'].astype('int')\n",
    "\n",
    "# Merge BS and Searchterm IS Report\n",
    "merged_sb = pd.merge(merged_sb, sb_is_clean, left_on=['Campaign Name','st_resolve'], right_on=['Campaign Name','Customer Search Term'], how='left').drop('Customer Search Term', axis=1)\n",
    "\n",
    "\n",
    "# Drop Columns Unnecessary in Targeting Report\n",
    "sb_targ_clean = pd.concat([sb_targ[['Campaign Name','Targeting','Match Type','Impressions','Cost Per Click (CPC)','Click-Thru Rate (CTR)','14 Day Conversion Rate','Total Return on Advertising Spend (ROAS)']],sbv_targ[['Campaign Name','Targeting','Match Type','Impressions','Cost Per Click (CPC)','Click-Thru Rate (CTR)','14 Day Conversion Rate','Total Return on Advertising Spend (ROAS)']]])\n",
    "sb_targ_clean.columns = ['Campaign Name','Targeting','Match Type','90D Impressions','90D CPC','90D CTR','90D CVR','90D ROAS']\n",
    "sb_targ_clean['st_resolve'] = sb_targ_clean.Targeting\n",
    "sb_targ_clean['Match Type'] = sb_targ_clean['Match Type'].str.lower()\n",
    "sb_targ_clean.drop('Targeting', axis=1, inplace=True)\n",
    "\n",
    "# Merge BS and Longterm Targeting KPI\n",
    "merged_sb = pd.merge(merged_sb, sb_targ_clean, on=['Campaign Name','st_resolve','Match Type'], how='left')\n",
    "\n",
    "merged_sb['Ad Group Name'] = merged_sb['Campaign Name']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine SP and SB Data before applying Recos\n",
    "merged = pd.concat([merged,merged_sb]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Bid Recommendations based on KPI and add to dataframe\n",
    "\n",
    "# Define Conditions for Flags and Recommendations\n",
    "cons = [\n",
    "    ((merged['90D CVR'].notna()) & (merged['90D CVR'] > 0) & (merged['ROAS'].notna()) & (merged['ROAS'] > 0.0) & (merged['BE ROAS'] > 0.0) & (merged['AMZ Price'] > 0.0) & (merged['ROAS'] <= merged['BE ROAS']) & (((merged['AMZ Price'] * merged['90D CVR']) / (merged['BE ROAS'] * merged['Percentage'])) <= merged['bid_resolve'])),\n",
    "    ((merged['ROAS'] > 0.0) & (merged['BE ROAS'] > 0.0) & (merged['ROAS'] <= merged['BE ROAS']) & ((merged['CPC'] / merged['Percentage']) <= merged['bid_resolve'])),  \n",
    "    ((merged['BE ROAS'] > 0.0) & (merged['ROAS'] >= merged['BE ROAS']) & (merged['Organic Rank'].notna()) & (merged['Sponsored Rank'].notna()) & (merged['Organic Mean'] > (merged['Organic Target']*1.05)) & (merged['Sponsored Mean'] > (merged['Sponsored Target']*1.05)) & (np.abs(merged['Sponsored Slope']) > 0.03) & (merged['Sponsored Slope'] < 0.0)),\n",
    "    ((merged['BE ROAS'] > 0.0) & (merged['ROAS'] >= merged['BE ROAS']) & (merged['Search Term Impression Rank'].notna()) & ((merged['Search Term Impression Rank']) > 2)),\n",
    "    ((merged['BE ROAS'] > 0.0) & (merged['ROAS'] >= merged['BE ROAS']) & (merged['CPC']>0.0) & (((merged['CPC']/merged['Percentage']) * 1.15) >= merged['bid_resolve'])),\n",
    "    ((merged['BE ROAS'] > 0.0) & (merged['ROAS'] >= merged['BE ROAS'])),\n",
    "    ((merged['90D Impressions'].notna()) & (merged['90D Impressions'] > 0.0) & (merged['90D CTR'] > 0.0) & (merged['90D CVR'].notna()) & (merged['90D CVR'] == 0) & (merged['Clicks'].notna()) & (merged['Clicks'] >= 8)),\n",
    "    ((merged['90D Impressions'] > 0.0) & (merged['Units'] == 0) & (merged['Impressions'] < 100) & (merged['Clicks'] < 5)),\n",
    "    ((merged['90D Impressions'] == 0.0) & (merged['Impressions'] == 0.0))\n",
    "]\n",
    "\n",
    "# Define FLag IDs for each Condition\n",
    "flags = (\n",
    "    '0901',\n",
    "    '0900',\n",
    "    '0401',\n",
    "    '0402',\n",
    "    '0402',\n",
    "    '0400',\n",
    "    '0700',\n",
    "    '0101',\n",
    "    '0100'\n",
    "    )\n",
    "\n",
    "# Define Flag Class for each Condition\n",
    "types = (\n",
    "    'Down',\n",
    "    'Down',\n",
    "    'Up',\n",
    "    'Up',\n",
    "    'Up',\n",
    "    'Up',\n",
    "    'Kill',\n",
    "    'Stale',\n",
    "    'Stale'\n",
    "    )\n",
    "\n",
    "# Define Flag Text for each Condition\n",
    "texts = (\n",
    "    'Unprofitable - Lower to Safe Bid',\n",
    "    'Unprofitable - Missing Historical Data',\n",
    "    'Profitable - Losing Sponsored Position',\n",
    "    'Profitable - Low Impression Share',\n",
    "    'Profitable - Competitive Bid', \n",
    "    'Profitable - Already Bidding High',\n",
    "    'Many Clicks With No Sales',\n",
    "    'Low Data - Raise Bid or Kill',\n",
    "    'No Impressions - Raise Bid 5%'\n",
    "    )\n",
    "\n",
    "# Formulas to calculate reccomended Bids, conditionally\n",
    "recs = (\n",
    "    np.maximum(np.minimum(((merged['AMZ Price'] * merged['90D CVR']) / (merged['BE ROAS'] * merged['Percentage'])),(merged['bid_resolve']*0.95)),0.02),\n",
    "    np.maximum(np.minimum((merged['CPC'] / merged['Percentage']),(merged['bid_resolve']*0.95)),0.02),\n",
    "    merged['bid_resolve']*1.2,\n",
    "    merged['bid_resolve']*1.2,\n",
    "    merged['bid_resolve']*1.15,\n",
    "    merged['bid_resolve']*1.10,\n",
    "    0.02,\n",
    "    merged['bid_resolve']*1.05,\n",
    "    merged['bid_resolve']*1.05\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Dataframe with proportional Shape to Merged DF\n",
    "# We are using Numpy.select() in series for each items based on the same conditions.\n",
    "# In testing this had significantly better performance than df.apply with lambda fns\n",
    "reco_df = pd.DataFrame({'Flag':np.select(cons,flags),\n",
    "            'Flag Type':np.select(cons,types),\n",
    "            'Flag Text':np.select(cons,texts),\n",
    "            'Recommended Bid':np.select(cons,recs)})\n",
    "\n",
    "# Merge (column-wise) Merged DF and Recommendations\n",
    "merged = pd.concat([merged,reco_df],axis=1)\n",
    "merged['Recommended Bid'] = merged['Recommended Bid'].round(2)\n",
    "merged['Max CPC'] = merged.bid_resolve * merged.Percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge previous history to audits\n",
    "\n",
    "\n",
    "# Read in History Csv\n",
    "hist_df = pd.read_csv(hist_path, encoding='UTF-8', parse_dates=['Date'], dtype=merged.dtypes.to_dict()).sort_values('Date').drop_duplicates(subset=['Entity','Campaign Id','Ad Group Id','Keyword Id (Read only)',\n",
    "                                                                                                                    'Product Targeting Id (Read only)','Keyword Text','Product Targeting Expression','Match Type'],\n",
    "                                                                                                                    keep='last').reset_index()\n",
    "hist_df['Change Date'] = hist_df['Date']\n",
    "hist_df['Change Reason'] = hist_df['Flag Text']\n",
    "hist_df['Change'] = hist_df.apply(lambda x: f'${x.Bid:,.2f} -> ${x[\"Will Set Bid to:\"]:,.2f}', axis=1)\n",
    "\n",
    "#Merge in Bid Edit History\n",
    "merged = pd.merge(merged, hist_df[['Entity','Campaign Id',\n",
    "                                'Keyword Id (Read only)','Product Targeting Id (Read only)',\n",
    "                                'Keyword Text','Product Targeting Expression','Match Type','Change Date','Change Reason','Change','Notes']],\n",
    "                                on=['Entity', 'Campaign Id', 'Keyword Id (Read only)', \n",
    "                                'Product Targeting Id (Read only)', 'Keyword Text', 'Product Targeting Expression','Match Type'],\n",
    "                                how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Product: Active Detergent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kneel/.local/lib/python3.10/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing\n",
      "Starting Product: Dishwasher Cleaner\n",
      "writing\n",
      "Starting Product: Washing Machine Cleaner\n",
      "writing\n",
      "Starting Product: IP Sealing Ring 3 QT\n",
      "writing\n",
      "Starting Product: IP Sealing Ring 8 QT\n",
      "writing\n",
      "Starting Product: Silicone Bakeware Sling\n",
      "writing\n",
      "Starting Product: Steam Diverter DUO\n",
      "writing\n",
      "Starting Product: Exp Steamer Basket\n",
      "writing\n",
      "Starting Product: Steam Diverter LUX\n",
      "writing\n",
      "Starting Product: Cakeware Pans 4 7 9\n",
      "writing\n",
      "Starting Product: Cakeware Pans 4 Inch\n",
      "writing\n",
      "Starting Product: Cakeware Pans 7 Inch\n",
      "writing\n",
      "Starting Product: Cakeware Pans 9 Inch\n",
      "writing\n",
      "Starting Product: IP Sealing Ring 6 QT\n",
      "writing\n",
      "Starting Product: Silicone Roasting Rack\n",
      "writing\n",
      "Starting Product: IP Lids 6 QT\n",
      "writing\n",
      "Starting Product: IP Lids 8 QT\n",
      "writing\n",
      "Starting Product: Silicone Steamer Basket\n",
      "writing\n",
      "Starting Product: Foodi Lid Stand\n",
      "writing\n",
      "Starting Product: Silicone Egg Bite Molds\n",
      "writing\n",
      "Starting Product: Plant Stands\n",
      "writing\n",
      "Starting Product: Arch Plant Stand\n",
      "writing\n",
      "Starting Product: Floating Shelves\n",
      "writing\n",
      "Starting Product: Cakeware Pans General\n",
      "writing\n"
     ]
    }
   ],
   "source": [
    "#Final Excel Audit Setup and Formating\n",
    "\n",
    "# Drop Rows with no recommendations and reorder\n",
    "audit = merged[merged.Flag != '0'][['Ad Group Name', 'Product Name', 'Ad Group Type', 'BE ROAS', 'Organic Target', 'Sponsored Target', 'DP CVR',\n",
    "        'Product', 'Entity', 'Campaign Id', 'Ad Group Id', 'Keyword Id (Read only)', 'Product Targeting Id (Read only)', 'Keyword Text', 'Product Targeting Expression',\n",
    "        'Campaign Name', 'st_resolve', 'Match Type', 'bid_resolve', 'Percentage', 'Max CPC', \n",
    "        'Impressions', 'Clicks', 'Orders', 'Units', 'Click-through Rate', 'Conversion Rate', 'Spend', 'Sales', 'CPC', 'ROAS', \n",
    "        '90D Impressions', '90D CTR', '90D CVR', '90D CPC', '90D ROAS',\n",
    "        'Search Term Impression Rank', 'Search Term Impression Share', 'Organic Rank', 'Sponsored Rank', 'Organic Mean', 'Sponsored Mean',\n",
    "        'Change Date','Change Reason','Change','Notes','Flag', 'Flag Type', 'Flag Text', 'Recommended Bid']].reset_index(drop=True)\n",
    "\n",
    "\n",
    "#Reassign Column Names to show in spreadsheet\n",
    "audit.columns = ['Ad Group','Product Name', 'AG Type', 'BE ROAS', 'Organic Target', 'Sponsored Target', 'DP CVR',\n",
    "        'Product', 'Entity', 'Campaign Id', 'Ad Group Id', 'Keyword Id (Read only)', 'Product Targeting Id (Read only)', 'Keyword Text', 'Product Targeting Expression',\n",
    "        'Campaign', 'Target', 'Match Type', 'Bid', 'Mult.', 'Max CPC', \n",
    "        'Impressions', 'Clicks', 'Orders', 'Units', 'CTR', 'CVR', 'Spend', 'Sales', 'CPC', 'ROAS', \n",
    "        '90D Impressions', '90D CTR', '90D CVR', '90D CPC', '90D ROAS',\n",
    "        'IS Rank', 'IS %', 'Org. Rank', 'Spo. Rank', 'Org. Mean', 'Spo. Mean','Change Date','Change Reason','Change','Notes',\n",
    "        'Flag', 'Flag Type', 'Flag Text', 'Reco. Bid']\n",
    "\n",
    "# Add Actions Columns\n",
    "audit['Skip?'] = False\n",
    "audit['Override Bid'] = ''\n",
    "audit['Will Set Bid to:'] = '=IF(NOT(ISBLANK(INDIRECT(\"RC[-1]\",FALSE()))),IF(NOT(INDIRECT(\"RC[-2]\",FALSE())),INDIRECT(\"RC[-1]\",FALSE()),\"\"),IF(NOT(INDIRECT(\"RC[-2]\",FALSE())),INDIRECT(\"RC[-3]\",FALSE()),\"\"))'\n",
    "audit['Multiplier'] = audit['Mult.']\n",
    "audit['New CPC'] = '=IFERROR(ROUND(INDIRECT(\"RC[-1]\",FALSE)*INDIRECT(\"RC[-2]\",FALSE),2),\"\")'\n",
    "audit ['AMZ Link']= '=IF(INDIRECT(\"RC[-47]\",0)=\"Keyword\",HYPERLINK(\"https://www.amazon.com/s?k=\"&SUBSTITUTE(INDIRECT(\"RC[-39]\",0),\" \",\"+\")),IF(INDIRECT(\"RC[-47]\",0)=\"Product Targeting\",HYPERLINK(\"https://www.amazon.com/dp/\"&INDIRECT(\"RC[-39]\",0)),\"\"))'\n",
    "#set lists to iterate later\n",
    "adtypes = ag_df['Ad Type'].unique()\n",
    "agtypes = ag_df['Ad Group Type'].unique()\n",
    "flagtypes = ['Up','Down','Kill','Stale']\n",
    "\n",
    "#Short codes for ad types\n",
    "short = {'Sponsored Products':'SP','Sponsored Brands': 'SB'}\n",
    "\n",
    "# # Set Number Formats in Dataframe because it's less memory intensive than setting them in the spreadsheet\n",
    "# format_mapping = {\n",
    "#     'BE ROAS':'{:.2f}',\n",
    "#     'Bid':\"${:.2f}\", \n",
    "#     'Mult.': \"{:.0%}\", \n",
    "#     'Max CPC':\"${:.2f}\", \n",
    "#     'Impressions': \"{:,.0f}\", \n",
    "#     'Clicks': \"{:,.0f}\", \n",
    "#     'Orders': \"{:,.0f}\", \n",
    "#     'Units': \"{:,.0f}\", \n",
    "#     'CTR': '{:.2%}', \n",
    "#     'CVR': '{:.2%}', \n",
    "#     'Spend':\"${:,.2f}\", \n",
    "#     'Sales':\"${:,.2f}\", \n",
    "#     'CPC':\"${:.2f}\", \n",
    "#     'ROAS':'{:.2f}', \n",
    "#     '90D Impressions': \"{:,.0f}\", \n",
    "#     '90D CTR': '{:.2%}', \n",
    "#     '90D CVR': '{:.2%}', \n",
    "#     '90D CPC':\"${:,.2f}\", \n",
    "#     '90D ROAS':'{:.2f}',\n",
    "#     'IS %': '{:.2%}', \n",
    "#     'Org. Mean':'{:.2f}', \n",
    "#     'Spo. Mean':'{:.2f}',\n",
    "#     'Reco. Bid':\"${:.2f}\",\n",
    "#     'Multiplier': \"{:.0%}\"\n",
    "# }\n",
    "# for key, value in format_mapping.items():\n",
    "#     audit[key] = audit[key].apply(value.format)\n",
    "\n",
    "# Iterate thru products that exist\n",
    "for product in prod_df['Product Name']:\n",
    "    if audit['Product Name'].isin([product]).any().any():\n",
    "        print(f'Starting Product: {product}')\n",
    "        exist=None\n",
    "        # Generate spreadsheet and product level details\n",
    "        wb = load_workbook(filename=f'{confdir}sprint_template.xlsx')\n",
    "        checklist = wb['Checklist']\n",
    "        this_sh = wb['Temp']\n",
    "        next_sh = wb.copy_worksheet(this_sh)\n",
    "        \n",
    "        # slice of audit for this product\n",
    "        prod_aud = audit[audit['Product Name'] == product]\n",
    "        ssname = f'Generated_Audits/{product}_{td_form}.xlsx'\n",
    "\n",
    "        # Static Attributes for Product\n",
    "        be = prod_df.loc[(prod_df['Product Name'] == product),'BE ROAS'].item()\n",
    "        dpcr = prod_df.loc[(prod_df['Product Name'] == product),'DP CVR'].item()\n",
    "        og = prod_df.loc[(prod_df['Product Name'] == product),'Organic Target'].item()\n",
    "        sg = prod_df.loc[(prod_df['Product Name'] == product),'Sponsored Target'].item()\n",
    "        \n",
    "        # Iterate through Ad Type (SP, SB, etc.) and Ad group type (KW Exact, Competitor, Auto, etc.) and Flag type (Up Bid, Down Bid, Kill, etc.)\n",
    "        for adtype in adtypes:\n",
    "            for agtype in agtypes:\n",
    "                for flagtype in flagtypes:\n",
    "                    # Create slice for this combo\n",
    "                    working = prod_aud.loc[(prod_aud['Product'] == adtype) & (prod_aud['AG Type'] == agtype) & (prod_aud['Flag Type'] == flagtype)]\n",
    "                    working = working.sort_values(by='Spend',ascending=False).replace(np.nan, '', regex=True).astype('object')\n",
    "                    l,w = working.shape\n",
    "                    if (l>=1):\n",
    "                        exist=True\n",
    "                        #Set Top of Audit Static Details\n",
    "                        this_sh['S1'] = product\n",
    "                        this_sh['S2'] = adtype\n",
    "                        this_sh['S3'] = agtype\n",
    "                        this_sh['S4'] = flagtype\n",
    "                        this_sh['Y1'] = be\n",
    "                        this_sh['Y2'] = dpcr\n",
    "                        this_sh['Y3'] = og\n",
    "                        this_sh['Y4'] = sg\n",
    "                        this_sh['Y5'] = l\n",
    "\n",
    "                        #Add Audit to Checklist\n",
    "                        checklist.append([f'{short[adtype]} - {agtype} - {flagtype}'])\n",
    "\n",
    "                        # Name current Tab\n",
    "                        this_sh.title = f'{short[adtype]} - {agtype} - {flagtype}'\n",
    "                        \n",
    "                        # Write Slice to the sheet and format\n",
    "                        rows = dataframe_to_rows(working, index=False, header=True)\n",
    "                        for r_idx, row in enumerate(rows, 1):\n",
    "                            for c_idx, value in enumerate(row, 1):\n",
    "                                this_sh.cell(row=r_idx+6, column=c_idx, value=value)\n",
    "\n",
    "                        #Activate next sheet\n",
    "                        this_sh = next_sh\n",
    "                        next_sh = wb.copy_worksheet(this_sh)\n",
    "\n",
    "        #Write Audit File\n",
    "        if(exist):\n",
    "            print('writing')\n",
    "            #Remove extra copied sheets before writing\n",
    "            for sn in wb.sheetnames:\n",
    "                if 'Copy' in sn:\n",
    "                    wb.remove(wb[sn])\n",
    "            wb.save(filename=ssname)\n",
    "            wb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
